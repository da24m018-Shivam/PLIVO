# PII NER â€“ STT-Based Synthetic Data + Token Classification Model

This repository contains my complete solution for the **IIT Madras â€“ PII NER Assignment (2025)**.

The goal is to build a lightweight, low-latency Named-Entity Recognizer for detecting PII entities from noisy, STT-style (speech-to-text) utterances, using:

- Synthetic data generation
- Token classification models (HuggingFace Transformers)
- BIO tagging + span evaluation
- Latency measurement (batch_size = 1)
- Robust post-processing for PII precision

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl                # synthetic STT-style train dataset
â”‚   â”œâ”€â”€ dev.jsonl                  # synthetic STT-style dev dataset
â”‚   â””â”€â”€ test.jsonl                 # original assignment test set
â”‚
â”œâ”€â”€ out/                           # saved model + predictions + metrics
â”‚   â”œâ”€â”€ dev_pred.json
â”‚   â””â”€â”€ test_pred.json
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ dataset.py                 # dataset â†’ tokenization â†’ BIO labels
    â”œâ”€â”€ labels.py                  # label â†” id mappings (BIO scheme)
    â”œâ”€â”€ model.py                   # RoBERTa token classification model
    â”œâ”€â”€ train.py                   # training loop + optimizer + scheduler
    â”œâ”€â”€ predict.py                 # inference + BIO â†’ span decoding
    â”œâ”€â”€ eval_span_f1.py            # span-level F1 (per-entity + PII)
    â”œâ”€â”€ measure_latency.py         # p50 / p95 latency measurement
    â”œâ”€â”€ generate_synthetic_data.py # STT-style synthetic data generator
    â””â”€â”€ validate_data.py           # integrity checker for synthetic dataset
```

---

## ğŸš€ Approach Summary

### 1ï¸âƒ£ Synthetic Data Generation (STT-focused)

Since the assignment requires speech-style PII, I built a custom generator:

âœ“ Indian names, cities, locations  
âœ“ Mixed email formats ("dot", "at", no punctuation, spelled-out)  
âœ“ Phone numbers in multiple formats (digits, spelled-out, +91, grouped)  
âœ“ Realistic STT noise:
- "g male", "dilli", "bumbai", "varma/sharmma"
- fillers ("uh", "umm", "like", "yaar")
- light typos
- homophones ("too/to", "one/won")

âœ“ Noise added only to non-PII spans (entity text is preserved)  
âœ“ Spans recalculated after noise injection

This ensures realistic ASR noise without destroying label alignment.

**Generate synthetic data:**

```bash
python src/generate_synthetic_data.py --gen-train --gen-dev
```

---

### 2ï¸âƒ£ Model Choice

After testing DistilBERT, BERT-base, and RoBERTa-base, the final selected model is:

â­ **roberta-large**

- Highest PII F1 among tested models
- Latency still meets assignment constraints
- Robust BPE tokenizer handles noisy STT text well
- Excellent performance on EMAIL, CREDIT_CARD, PHONE

---

### 3ï¸âƒ£ Training

Runs a standard token classification fine-tuning loop:

- Batch size: 32
- Epochs: 13
- LR: 2e-5
- Max length: 128
- Warmup + linear scheduler
- Gradient clipping (1.0)

**Train command:**

```bash
python src/train.py --model_name roberta-large --train data/train.jsonl --dev data/dev.jsonl --out_dir out
```

The fine-tuned model + tokenizer are saved into `out/`.

---

### 4ï¸âƒ£ Inference + Robust PII Post-Processing

After BIO decoding, extra steps improve PII precision:

- Remove 1-character spans
- Validate CREDIT_CARD by numeric length (13â€“19 digits)
- Validate PHONE (7â€“15 digits)
- Normalize STT emails:
  - `ramesh dot sharma at gmail dot com` â†’ `ramesh.sharma@gmail.com`
- Validate EMAIL with regex
- Keep CITY, LOCATION, PERSON_NAME as-is
- Mark all PII spans with `"pii": true`

**Inference command:**

```bash
python src/predict.py --model_dir out --input data/dev.jsonl --output out/dev_pred.json
```

---

### 5ï¸âƒ£ Evaluation (Span F1)

Span evaluator checks exact start/end + label match.

Reports:
- Per-entity F1
- PII-only precision/recall/F1 (main metric)
- Non-PII metrics
- Macro F1

**Run evaluation:**

```bash
python src/eval_span_f1.py --gold data/dev.jsonl --pred out/dev_pred.json
```

---

### 6ï¸âƒ£ Latency Measurement

Latency run uses:
- batch_size = 1
- 50 runs
- forward pass + tokenization latency
- p50 and p95 reported

**Command:**

```bash
python src/measure_latency.py --model_dir out --input data/dev.jsonl --runs 50
```

---

## ğŸ“Š Final Metrics (Reported in Loom)

You will include:

- Per-entity F1
- PII Precision
- PII Recall
- PII F1 (main score)
- Overall Macro F1
- Latency p50 / p95 (ms)

These come directly from `eval_span_f1.py` and `measure_latency.py`.

---

## ğŸ§ª Test Predictions

Generate predictions for the assignment test set:

```bash
python src/predict.py --model_dir out --input data/test.jsonl --output out/test_pred.json
```

`out/test_pred.json` is included as part of the final submission.

---

## ğŸ¥ Loom Video Checklist

Your Loom should cover:

- âœ” Final results + metrics
- âœ” Codebase walkthrough (src/* overview)
- âœ” Synthetic data generation logic
- âœ” Model & tokenizer selection
- âœ” Key hyperparameters
- âœ” PII precision/recall/F1 explanation
- âœ” Latency trade-offs (p50/p95)

---

## ğŸ“Œ How to Reproduce Entire Pipeline

```bash
# 1. Generate synthetic data
python src/generate_synthetic_data.py --gen-train --gen-dev

# 2. Train RoBERTa-large
python src/train.py --model_name roberta-base --train data/train.jsonl --dev data/dev.jsonl --out_dir out

# 3. Predict on dev
python src/predict.py --model_dir out --input data/dev.jsonl --output out/dev_pred.json

# 4. Evaluate span F1
python src/eval_span_f1.py --gold data/dev.jsonl --pred out/dev_pred.json

# 5. Measure latency
python src/measure_latency.py --model_dir out --input data/dev.jsonl --runs 50

# 6. Predict on test
python src/predict.py --model_dir out --input data/test.jsonl --output out/test_pred.json
```
